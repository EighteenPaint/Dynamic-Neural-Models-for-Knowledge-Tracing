{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LERqDO59VeaM",
    "outputId": "1d48b66a-4bbb-4967-f2e5-6de7f6b78137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version = 2.4.0\n",
      "D:\\Jupyter_notebook\\Marina\\Code\\EDM_conf\n",
      "current dir : D:\\Jupyter_notebook\\Marina\\Code\\EDM_conf\n",
      "rootdir : \\Jupyter_notebook\\Marina\\Datasets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, Activation, Dense, \\\n",
    "                                    Flatten, Dropout, Conv1D, Reshape, BatchNormalization,\\\n",
    "                                        GRU, Bidirectional,SpatialDropout1D, GaussianDropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.initializers import Constant, RandomUniform\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print('Tensorflow version = {}'.format(tf.__version__))\n",
    "print(os.getcwd())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "import sys\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "rootdir = \"\\Jupyter_notebook\\Marina\\Datasets\"\n",
    "print(\"current dir : {}\".format(os.getcwd()))\n",
    "print(\"rootdir : {}\".format(rootdir) )\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "YIW1W1AoV84d"
   },
   "outputs": [],
   "source": [
    "#USE_W2V = True\n",
    "USE_W2V = False \n",
    "#emb_size = 300\n",
    "emb_size = 100\n",
    "#w2v_emb_size = 300 \n",
    "w2v_emb_size = 100\n",
    "L = 50\n",
    "max_epochs = 30\n",
    "beta = 1e-3\n",
    "#mod =  'tdnn_model' \n",
    "mod =  'bigru_model'\n",
    "spatd = None\n",
    "gausd = None\n",
    "if mod == 'bigru_model':\n",
    "    num_hidden = [50,25]\n",
    "    batch_size = 32    \n",
    "elif mod == 'tdnn_model':\n",
    "    num_hidden = [20,15,10,5]\n",
    "    batch_size = 50\n",
    "    \n",
    "use_sigmoid = False\n",
    "CREATE_NEW_DATA_SPLIT = True\n",
    "global fold\n",
    "    \n",
    "#DATASET = 'assist2009_corrected'\n",
    "#DATASET = 'assist2009_updated'\n",
    "DATASET = 'fsaif1tof3'\n",
    "#DATASET = 'assistment2012_13'\n",
    "#DATASET = 'assistment2017'\n",
    "\n",
    "max_v_auc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `printProgressBar`: function that prints progress bar in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "hfL7IhFIWDPL"
   },
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '',\n",
    "                      decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd, flush=True)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "79P2REsXWHx3"
   },
   "outputs": [],
   "source": [
    "#Read file in 3-lines format and return `data` numpy array\n",
    "\n",
    "def read_file_3lines(file, start_user):\n",
    "    user_ids = []\n",
    "    skill_ids = []\n",
    "    correct = []\n",
    "    with open(file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        cnt = 0\n",
    "        user_id = start_user\n",
    "        try:\n",
    "            num_responses = int(line)\n",
    "        except:\n",
    "            print('Error')\n",
    "        user_ids += [user_id]*num_responses\n",
    "        while line:\n",
    "            line = f.readline()\n",
    "            if line==\"\":\n",
    "                break\n",
    "            cnt += 1\n",
    "            if cnt%3 == 0:\n",
    "                user_id += 1\n",
    "                num_responses = int(line)\n",
    "                user_ids += [user_id]*num_responses\n",
    "            elif cnt%3 == 1:\n",
    "                skill_ids += line.replace(\"\\n\",\"\").split(\",\")\n",
    "            elif cnt%3==2:\n",
    "                correct += line.replace(\"\\n\",\"\").split(\",\")\n",
    "        user_ids = np.reshape(np.array(user_ids),[-1,1])\n",
    "        num_unique_users = np.unique(user_ids[:,0]).shape[0]\n",
    "        skill_ids = np.reshape(np.array(skill_ids).astype(int),[-1,1])\n",
    "        correct = np.reshape(np.array(correct).astype(int),[-1,1])\n",
    "        idx = np.reshape((correct==0) + (correct==1), [-1])\n",
    "        data = np.hstack((user_ids[idx], skill_ids[idx], correct[idx]))\n",
    "        return data, num_unique_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "1I9k1UNpWO-4"
   },
   "outputs": [],
   "source": [
    "def gen_inputs_targets(data, user_ids, N, prefix):\n",
    "    printProgressBar(0, N, prefix = prefix, suffix = 'Complete', length = 50)\n",
    "    \n",
    "    x = None\n",
    "    t = None\n",
    "    start = True\n",
    "    for i,student_id in enumerate(user_ids):\n",
    "        # Make an array with all the data for this student\n",
    "        student_data = data[data[:,0]==student_id]\n",
    "        skill_hist = toeplitz(student_data[:,1],0.0*np.ones([1,L]))\n",
    "        responses_hist = toeplitz(student_data[:,2],0.0*np.ones([1,L]))\n",
    "        student_data = np.hstack((skill_hist,\n",
    "                                np.fliplr(responses_hist)\n",
    "                                ))\n",
    "        if start:\n",
    "            start = False\n",
    "            x = student_data[1:,0:2*L-1]\n",
    "            t = student_data[1:,2*L-1].reshape([-1,1])\n",
    "        else:\n",
    "            x = np.vstack((x, student_data[1:,0:2*L-1]))\n",
    "            t = np.vstack((t, student_data[1:,2*L-1].reshape([-1,1])))\n",
    "        printProgressBar(i+1, N, prefix = prefix, suffix = 'Complete', length = 50)        \n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "RQiNtgbcWTSf"
   },
   "outputs": [],
   "source": [
    "#Read Train-Validation datasets\n",
    "\n",
    "def read_data(DATASET = 'assist2009_updated', I=None):\n",
    "    if (DATASET == 'assist2009_corrected') :\n",
    "        train_file = path.join(rootdir,DATASET,\"assistment_2009_corrected_train{}.csv\".format(I))\n",
    "        valid_file = path.join(rootdir,DATASET,\"assistment_2009_corrected_valid{}.csv\".format(I))\n",
    "        test_file = path.join(rootdir,DATASET,\"assistment_2009_corrected_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        emb_file = path.join(rootdir, DATASET,'skill_name_embeddings_corrected_300d.csv') \n",
    "        #emb_file = path.join(rootdir, DATASET,'Assistment_2009_corrected_skname_embeddings_FastText.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET, 'skill_name_embeddings_corrected_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_corrected.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values \n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.6 \n",
    "            \n",
    "    elif DATASET == 'assist2009_updated':\n",
    "        train_file = path.join(rootdir,DATASET,\"assist2009_updated_train{}.csv\".format(I))\n",
    "        valid_file = path.join(rootdir,DATASET,\"assist2009_updated_valid{}.csv\".format(I))\n",
    "        test_file = path.join(rootdir,DATASET,\"assist2009_updated_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        #emb_file = path.join(rootdir, DATASET, 'skill_name_embeddings_updated_300d.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET, 'Assist2009_updated_skname_embeddings_FastText.csv')\n",
    "        emb_file = path.join(rootdir, DATASET, 'skill_name_embeddings_updated_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_updated.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values \n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.6\n",
    "        \n",
    "    elif DATASET == 'fsaif1tof3': \n",
    "        train_file = path.join(rootdir,DATASET,\"fsaif1tof3_train{}.csv\".format(I))\n",
    "        valid_file = path.join(rootdir,DATASET,\"fsaif1tof3_valid{}.csv\".format(I))\n",
    "        test_file = path.join(rootdir,DATASET,\"fsaif1tof3_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        #emb_file = path.join(rootdir, DATASET,'fsaif1tof3_embeddings_300d.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET,'fsaif1tof3_skname_embeddings_FastText.csv')\n",
    "        emb_file = path.join(rootdir, DATASET,'fsaif1tof3_embeddings_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_name_question_id.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values \n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.9\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.9\n",
    "        \n",
    "    elif DATASET == 'assistment2012_13':\n",
    "        train_file = path.join(rootdir,DATASET,\"assistment2012_13_3lines_train{}_3lines.csv\".format(I))\n",
    "        valid_file = path.join(rootdir,DATASET,\"assistment2012_13_3lines_valid{}_3lines.csv\".format(I))\n",
    "        test_file = path.join(rootdir,DATASET,\"assistment2012_13_3lines_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        #emb_file = path.join(rootdir, DATASET,'skill_name_embeddings_12_13_300d.csv') \n",
    "        emb_file = path.join(rootdir, DATASET,'Assistment2012_13_skname_embeddings_FastText.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET,'skill_name_embeddings_12_13_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_12_13.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values \n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.2        \n",
    "        \n",
    "    elif DATASET == 'assistment2017':\n",
    "        train_file = path.join(rootdir,DATASET,\"assistment2017_train{}.csv\".format(I))\n",
    "        valid_file = path.join(rootdir,DATASET,\"assistment2017_valid{}.csv\".format(I))\n",
    "        test_file = path.join(rootdir,DATASET,\"assistment2017_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        #emb_file = path.join(rootdir, DATASET, 'Assistment2017_skill_names_embeddings_300d.csv')\n",
    "        emb_file = path.join(rootdir, DATASET, 'assistment20017_skname_embeddings_FastText.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET, 'Assistment2017_skill_names_embeddings_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_assistment2017.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values\n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.4        \n",
    "        \n",
    "    else :\n",
    "        print ('Dataset file not found')\n",
    "    \n",
    "    num_skills = skill_names.shape[0]     \n",
    "    \n",
    "    # Add a zero row at the beginning\n",
    "    if USE_W2V:\n",
    "        embeddings = pd.read_csv(emb_file, sep=',', header=None)#\n",
    "        embeddings = np.vstack((np.zeros([1,w2v_emb_size]), embeddings))\n",
    "    else:\n",
    "        embeddings = np.zeros([num_skills,emb_size])\n",
    "        embeddings = np.vstack((np.zeros([1,emb_size]), embeddings))\n",
    "    \n",
    "    start_user = 1\n",
    "    data_train, N_train = read_file_3lines(train_file, start_user)\n",
    "    start_user += N_train\n",
    "    data_valid, N_valid = read_file_3lines(valid_file, start_user)\n",
    "    start_user += N_valid\n",
    "    data_test, N_test = read_file_3lines(test_file, start_user)\n",
    "    \n",
    "    return data_train, data_test, data_valid, embeddings, skill_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QOnnR46SWXH1"
   },
   "outputs": [],
   "source": [
    "#Read Train-Test datasetd\n",
    "\n",
    "def read_data_test(DATASET = 'assist2009_corrected'):\n",
    "    if (DATASET == 'assist2009_corrected') :\n",
    "        train_file = path.join(rootdir,DATASET,\"assistment_2009_corrected_train.csv\")\n",
    "        test_file = path.join(rootdir,DATASET,\"assistment_2009_corrected_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        emb_file = path.join(rootdir, DATASET,'skill_name_embeddings_corrected_300d.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET,'Assistment_2009_corrected_skname_embeddings_FastText.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET, 'skill_name_embeddings_corrected_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_corrected.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values\n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.6\n",
    "               \n",
    "    elif DATASET == 'assist2009_updated':\n",
    "        train_file = path.join(rootdir,DATASET,\"assist2009_updated_train.csv\")\n",
    "        test_file = path.join(rootdir,DATASET,\"assist2009_updated_test.csv\")\n",
    "        \n",
    "        # Read embedding data        \n",
    "        #emb_file = path.join(rootdir, DATASET, 'skill_name_embeddings_updated_300d.csv')\n",
    "        emb_file = path.join(rootdir, DATASET, 'Assist2009_updated_skname_embeddings_FastText.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET, 'skill_name_embeddings_updated_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_updated.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values\n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.6\n",
    "        \n",
    "    elif DATASET == 'fsaif1tof3': \n",
    "        train_file = path.join(rootdir,DATASET,\"fsaif1tof3_train.csv\")\n",
    "        test_file = path.join(rootdir,DATASET,\"fsaif1tof3_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        emb_file = path.join(rootdir, DATASET,'fsaif1tof3_embeddings_300d.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET,'fsaif1tof3_skname_embeddings_FastText.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET,'fsaif1tof3_embeddings_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_name_question_id.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, header=None).values\n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.9\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.9        \n",
    "        \n",
    "    elif DATASET == 'assistment2012_13':\n",
    "        train_file = path.join(rootdir,DATASET,\"assistment2012_13_train.csv\")\n",
    "        test_file = path.join(rootdir,DATASET,\"assistment2012_13_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        #emb_file = path.join(rootdir, DATASET,'skill_name_embeddings_12_13_300d.csv') \n",
    "        #emb_file = path.join(rootdir, DATASET,'Assistment2012_13_skname_embeddings_FastText.csv')\n",
    "        emb_file = path.join(rootdir, DATASET,'skill_name_embeddings_12_13_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_12_13.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values \n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.2\n",
    "    \n",
    "    elif DATASET == 'assistment2017':\n",
    "        train_file = path.join(rootdir,DATASET,\"assistment2017_train.csv\")\n",
    "        test_file = path.join(rootdir,DATASET,\"assistment2017_test.csv\")\n",
    "        \n",
    "        # Read embedding data\n",
    "        #emb_file = path.join(rootdir, DATASET, 'Assistment2017_skill_names_embeddings_300d.csv')\n",
    "        #emb_file = path.join(rootdir, DATASET, 'assistment20017_skname_embeddings_FastText.csv')\n",
    "        emb_file = path.join(rootdir, DATASET, 'Assistment2017_skill_names_embeddings_100d.csv')\n",
    "        \n",
    "        # Read skill names\n",
    "        sknames_file = path.join(rootdir, DATASET,'skill_names_assistment2017.csv')\n",
    "        skill_names = pd.read_csv(sknames_file, sep=',', header=None).values\n",
    "        \n",
    "        if mod == 'bigru_model':\n",
    "            spatd = 0.2\n",
    "            gausd = 0.2\n",
    "        elif mod == 'tdnn_model':\n",
    "            spatd = 0.5\n",
    "            gausd = 0.4\n",
    "    else :\n",
    "        print ('Dataset file not found')    \n",
    "        \n",
    "    num_skills = skill_names.shape[0]\n",
    "    \n",
    "    # Add a zero row at the beginning\n",
    "    if USE_W2V:\n",
    "        embeddings = pd.read_csv(emb_file, sep=',', header=None)\n",
    "        embeddings = np.vstack((np.zeros([1,w2v_emb_size]), embeddings))\n",
    "    else:\n",
    "        embeddings = np.zeros([num_skills,emb_size])\n",
    "        embeddings = np.vstack((np.zeros([1,emb_size]), embeddings))\n",
    "        \n",
    "    start_user = 1\n",
    "    data_train, N_train = read_file_3lines(train_file, start_user)\n",
    "    start_user += N_train\n",
    "    data_test, N_test = read_file_3lines(test_file, start_user)\n",
    "    return data_train, data_test, embeddings, skill_names, spatd, gausd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UyuCVa60WoLu"
   },
   "outputs": [],
   "source": [
    "global keys\n",
    "global key_val_acc\n",
    "global key_val_auc\n",
    "global key_acc\n",
    "global key_auc\n",
    "\n",
    "def get_key(keystart, list):\n",
    "    for k in list:\n",
    "        if k[:len(keystart)] == keystart:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global keys\n",
    "        global key_val_acc\n",
    "        global key_val_auc\n",
    "        global key_acc\n",
    "        global key_auc\n",
    "        global filename\n",
    "        if keys==[]:\n",
    "            keys = list(logs.keys())\n",
    "            key_val_acc = get_key('val_acc', keys)\n",
    "            key_val_auc = get_key('val_auc', keys)\n",
    "            key_acc = get_key('acc', keys)\n",
    "            key_auc = get_key('auc', keys)\n",
    "            filename = 'tdnn_w2v-f{:d}'.format(fold)\\\n",
    "                +'-e{epoch:02d}'\\\n",
    "                +'-val_loss{val_loss:.4f}-val_accuracy{val_accuracy:.4f}'\\\n",
    "                +'-val_auc{'+key_val_auc+':.4f}'\\\n",
    "                +'.h5'\n",
    "            checkpoint.filepath = filename\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, beta):\n",
    "    if epoch < 10:\n",
    "        return beta\n",
    "    else:\n",
    "        return beta * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "callback1 = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "PespGhQkWw8L"
   },
   "outputs": [],
   "source": [
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bigru(num_hidden = [10], use_sigmoid=False):\n",
    "    num_hidden = num_hidden+[1]    # add the extra output layer\n",
    "    num_layers = len(num_hidden)\n",
    "    # Inputs\n",
    "    q_ids = Input(shape=[L], dtype=tf.int32)\n",
    "    hist = Input(shape=[L-1])\n",
    "    \n",
    "    if USE_W2V:\n",
    "        print(\"!!!!!!!!!!!! Using Pre-trained Skil name Embeddings!!!!\")\n",
    "        initial_emb = Constant(embeddings/(L*w2v_emb_size))\n",
    "        q = Embedding(embeddings.shape[0], w2v_emb_size, embeddings_initializer=initial_emb,mask_zero=True)(q_ids)\n",
    "        \n",
    "        initial_h_emb = RandomUniform(minval=-1/(w2v_emb_size*L), maxval=1/(w2v_emb_size*L))\n",
    "        hist_emb = Embedding(2, w2v_emb_size, embeddings_initializer=initial_h_emb)(hist)\n",
    "        \n",
    "    else:\n",
    "        print(\"!!!!!!!!!!!! Using Random Skil name Embeddings!!!!\")\n",
    "        initial_emb = RandomUniform(minval=-1/(emb_size*L),maxval=1/(emb_size*L))\n",
    "        q = Embedding(embeddings.shape[0], emb_size, embeddings_initializer=initial_emb)(q_ids) \n",
    "        \n",
    "        initial_h_emb = RandomUniform(minval=-1/(emb_size*L), maxval=1/(emb_size*L))\n",
    "        hist_emb = Embedding(2, emb_size, embeddings_initializer=initial_h_emb)(hist)\n",
    "    \n",
    "    \n",
    "    print('q before conv:', q.shape)\n",
    "    print('hist before conv:', hist.shape)\n",
    "    \n",
    "    q = tf.keras.layers.SpatialDropout1D(spatd)(q) \n",
    "    q_conv = Conv1D(filters=100, kernel_size=3, strides=1)(q)\n",
    "    q_conv = BatchNormalization()(q_conv)    \n",
    "    q_conv = Activation(\"relu\")(q_conv)    \n",
    "        \n",
    "    hist_emb = tf.keras.layers.SpatialDropout1D(spatd)(hist_emb)  \n",
    "    hist_conv = Conv1D(filters=100, kernel_size=3, strides=1)(hist_emb)\n",
    "    hist_conv = BatchNormalization()(hist_conv)    \n",
    "    hist_conv = Activation(\"relu\")(hist_conv)   \n",
    "        \n",
    "    x = Concatenate(axis=1)([q_conv, hist_conv])\n",
    "    \n",
    "    x = Bidirectional(GRU(units=64, return_sequences=False))(x)\n",
    "\n",
    "    x = GaussianDropout(gausd)(x)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        if layer == num_layers-1:\n",
    "            activation = \"sigmoid\"            \n",
    "        else:\n",
    "            activation = \"relu\"                  \n",
    "            x = Dense(num_hidden[layer], activation=activation)(x)     \n",
    "    \n",
    "    out = Dense(1, activation=activation)(x)\n",
    "   \n",
    "    model = Model(inputs=[q_ids, hist], outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time delay model (TDNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "SyUr3zxWWa-L"
   },
   "outputs": [],
   "source": [
    "def model_tdnn(num_hidden = [10], use_sigmoid=False):\n",
    "    num_hidden = num_hidden+[1]    # add the extra output layer\n",
    "    num_layers = len(num_hidden)\n",
    "    # Inputs\n",
    "    q_ids = Input(shape=[L], dtype=tf.int32)\n",
    "    hist = Input(shape=[L-1])\n",
    "    \n",
    "    if USE_W2V:\n",
    "        print(\"!!!!!!!!!!!! Using Pre-trained Skil name Embeddings!!!!\")\n",
    "        initial_emb = Constant(embeddings/(L*w2v_emb_size))\n",
    "        q = Embedding(embeddings.shape[0], w2v_emb_size, embeddings_initializer=initial_emb,mask_zero=True)(q_ids)\n",
    "        \n",
    "        initial_h_emb = RandomUniform(minval=-1/(w2v_emb_size*L), maxval=1/(w2v_emb_size*L))\n",
    "        hist_emb = Embedding(2, w2v_emb_size, embeddings_initializer=initial_h_emb)(hist)\n",
    "        \n",
    "    else:\n",
    "        print(\"!!!!!!!!!!!! Using Random Skil name Embeddings!!!!\")\n",
    "        initial_emb = RandomUniform(minval=-1/(emb_size*L),maxval=1/(emb_size*L))\n",
    "        q = Embedding(embeddings.shape[0], emb_size, embeddings_initializer=initial_emb)(q_ids)\n",
    "        \n",
    "        initial_h_emb = RandomUniform(minval=-1/(emb_size*L), maxval=1/(emb_size*L))\n",
    "        hist_emb = Embedding(2, emb_size, embeddings_initializer=initial_h_emb)(hist)\n",
    "        \n",
    "    q = tf.keras.layers.SpatialDropout1D(spatd)(q)\n",
    "    q = Conv1D(50, 5)(q)\n",
    "    q = BatchNormalization()(q)    \n",
    "    q = Activation(\"relu\")(q)    \n",
    "    q = Flatten()(q)\n",
    "        \n",
    "    hist_emb = tf.keras.layers.SpatialDropout1D(spatd)(hist_emb)\n",
    "    hist_emb = Conv1D(50,5)(hist_emb)\n",
    "    hist_emb = BatchNormalization()(hist_emb)    \n",
    "    hist_emb = Activation(\"relu\")(hist_emb)   \n",
    "    hist_emb = Flatten()(hist_emb)\n",
    "    \n",
    "    x = Concatenate(axis=1)([q, hist_emb])\n",
    "    x = tf.keras.layers.GaussianDropout(gausd)(x)    \n",
    "   \n",
    "    for layer in range(num_layers):\n",
    "        if layer == num_layers-1:\n",
    "            activation = \"sigmoid\"\n",
    "            \n",
    "        else:\n",
    "            activation = \"relu\"                  \n",
    "            x = Dense(num_hidden[layer], activation=activation)(x)           \n",
    "   \n",
    "    out = Dense(1, activation=activation)(x)\n",
    "   \n",
    "    model = Model(inputs=[q_ids, hist], outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lHDmU-YaW0Eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===============EXPERIMENT NUMBER  1============\n",
      "~~~~~~~~~~~~~~~DATASET fsaif1tof3 ~~~~~~~~~~~\n",
      "Fold 1\n",
      "Number of skills: 2266\n",
      "Number of train students: 173\n",
      "Numberof validation students: 44\n",
      "Number of test students: 93\n",
      "(total: 310)\n",
      "Baseline valid accuracy = 0.45931353644427303\n",
      "==================================================\n",
      "L = 50, emb_size = 100, hidden=[50, 25], spatial dropout = None, gaussian dropout = None\n",
      "no model to delete\n",
      "~~~~~~~~~~Train Bi-GRU MODEL~~~~~~~~\n",
      "!!!!!!!!!!!! Using Random Skil name Embeddings!!!!\n",
      "q before conv: (None, 50, 100)\n",
      "hist before conv: (None, 49)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-cf5ca428e8d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bigru_model'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"~~~~~~~~~~Train Bi-GRU MODEL~~~~~~~~\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_bigru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_sigmoid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_sigmoid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-78393349c618>\u001b[0m in \u001b[0;36mmodel_bigru\u001b[1;34m(num_hidden, use_sigmoid)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgausd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 952\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1091\u001b[1;33m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    861\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\noise.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrate\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mnoised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main\n",
    "# Train-Validation\n",
    "\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(\" ===============EXPERIMENT NUMBER  {}============\".format(fold))\n",
    "    print(\"~~~~~~~~~~~~~~~DATASET {} ~~~~~~~~~~~\".format(DATASET))\n",
    "    keys = []\n",
    "    key_val_acc = None\n",
    "    key_val_auc = None\n",
    "    key_acc = None\n",
    "    key_auc = None\n",
    "      \n",
    "    if USE_W2V:\n",
    "        \n",
    "        data_split_file = path.join(rootdir,DATASET,\"{}_split_{}_w2v_L{}_emb_size={}.npz\".format(DATASET,fold,L,w2v_emb_size))\n",
    "    else:\n",
    "        data_split_file = path.join(rootdir,DATASET,\"{}_split_{}_not_w2v_L{}_emb_size={}.npz\".format(DATASET,fold,L,emb_size))\n",
    "        \n",
    "    \n",
    "    data_train, data_test,data_valid, embeddings, skill_names = read_data(DATASET=DATASET,I=fold)\n",
    "    skill_ids = np.unique(np.hstack((\n",
    "        data_train[:,1],\n",
    "        data_valid[:,1],\n",
    "        data_test[:,1]\n",
    "    )))\n",
    "    num_skills = len(skill_ids)\n",
    "    train_user_ids = np.unique(data_train[:,0])\n",
    "    valid_user_ids = np.unique(data_valid[:,0])\n",
    "    test_user_ids = np.unique(data_test[:,0])\n",
    "    N_train = len(train_user_ids)\n",
    "    N_valid = len(valid_user_ids)\n",
    "    N_test = len(test_user_ids)\n",
    "    num_students = N_train + N_test + N_valid\n",
    "    print('Fold {}'.format (fold))\n",
    "    print('Number of skills: {}'.format(num_skills))\n",
    "    print('Number of train students: {}'.format(N_train))\n",
    "    print('Numberof validation students: {}'.format(N_valid))\n",
    "    print('Number of test students: {}'.format(N_test))\n",
    "    print('(total: {})'.format(num_students))\n",
    "\n",
    "    # ### Generate `x_train`, `x_test`, `t_train`, `t_test`  \n",
    "    # Every student `stud_id` has a sequence of responses `correct[0], correct[1],..., correct[T-1]` for some skill `skill_id`. The length `T` of the sequence depends on the student and the skill.   \n",
    "    # Every row of `x_train` or `x_test` contains the `student_id`, the `skill_id` and the response `correct[t]` for some time `t`. In addition to that it also includes the history of length `L` of previous responses `correct[t-1],..., correct[t-L]`. These responses *must* correspond to the same student and the same skill as time `t`. If history is shorter than `L` then the missing entries are filled with `0`.\n",
    "\n",
    "    if not path.exists(data_split_file):\n",
    "        \n",
    "        #Generate Training, Validation and Testing data\n",
    "        \n",
    "        x_train, t_train = gen_inputs_targets(data_train,\n",
    "                                train_user_ids, N_train, 'Train set:')\n",
    "        x_valid, t_valid = gen_inputs_targets(data_valid,\n",
    "                                valid_user_ids, N_valid, 'Validation set:')\n",
    "        x_test, t_test = gen_inputs_targets(data_test,\n",
    "                                test_user_ids, N_test, 'Test set:')\n",
    "        \n",
    "        np.savez(data_split_file,\n",
    "                    embeddings = embeddings,\n",
    "                    x_train = x_train,\n",
    "                    x_valid = x_valid,\n",
    "                    x_test = x_test,\n",
    "                    t_train = t_train,\n",
    "                    t_valid = t_valid,\n",
    "                    t_test = t_test,\n",
    "                    train_user_ids = train_user_ids,\n",
    "                    valid_user_ids = valid_user_ids,\n",
    "                    test_user_ids = test_user_ids,\n",
    "                    N_train = N_train,\n",
    "                    N_valid = N_valid,\n",
    "                    N_test = N_test,\n",
    "                    num_skills = num_skills,\n",
    "                    num_students = num_students)    \n",
    "    else:\n",
    "        data_split = np.load(data_split_file)\n",
    "        embeddings = data_split['embeddings']\n",
    "        x_train = data_split['x_train']\n",
    "        x_test = data_split['x_test']\n",
    "        x_valid = data_split['x_valid']\n",
    "        t_train = data_split['t_train']\n",
    "        t_test = data_split['t_test']\n",
    "        t_valid = data_split['t_valid']\n",
    "        train_user_ids = data_split['train_user_ids']\n",
    "        valid_user_ids = data_split['valid_user_ids']\n",
    "        test_user_ids = data_split['test_user_ids']\n",
    "        N_train = data_split['N_train']\n",
    "        N_test = data_split['N_test']\n",
    "        N_valid = data_split['N_valid']\n",
    "        num_skills = data_split['num_skills']\n",
    "        num_students = data_split['num_students']\n",
    " \n",
    "    #Train the model\n",
    "\n",
    "    acc_valid_base = np.sum(t_valid==1)/t_valid.shape[0]\n",
    "    print('Baseline valid accuracy = {}'.format(acc_valid_base))   \n",
    "    print(\"==================================================\")\n",
    "    if USE_W2V:\n",
    "        print('L = {}, emb_size = {}, hidden={}, spatial dropout = {}, gaussian dropout = {}'.format(\n",
    "                L, w2v_emb_size, num_hidden, spatd, gausd))\n",
    "    else:\n",
    "        print('L = {}, emb_size = {}, hidden={}, spatial dropout = {}, gaussian dropout = {}'.format(\n",
    "                L, emb_size, num_hidden, spatd, gausd))\n",
    "     \n",
    "    try:        \n",
    "        del model \n",
    "        K.clear_session()\n",
    "    except:\n",
    "        print(\"no model to delete\")\n",
    "       \n",
    "       \n",
    "    if mod == 'tdnn_model':    \n",
    "        print(\"~~~~~~~Train TDNN MODEL~~~~~~~~~~\")\n",
    "        model = model_tdnn(num_hidden=num_hidden, use_sigmoid=use_sigmoid)    \n",
    "       \n",
    "        model.summary()   \n",
    "        model.compile(optimizer=Adamax(learning_rate=beta),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', AUC()])    \n",
    "   \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                        filepath=None)\n",
    "        my_callbacks = [\n",
    "            MyCallback(),        \n",
    "            checkpoint       \n",
    "            ]\n",
    "           \n",
    "        history = model.fit([x_train[:,:L].astype(int), x_train[:,L:]], t_train,\n",
    "                        validation_data=([x_valid[:,:L].astype(int), x_valid[:,L:]],t_valid),\n",
    "                        epochs = max_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=1)#,\n",
    "                        #callbacks=my_callbacks) #[ReduceLROnPlateau()]) #, [MyCallback()])\n",
    "\n",
    "    elif mod == 'bigru_model':    \n",
    "        print(\"~~~~~~~~~~Train Bi-GRU MODEL~~~~~~~~\")\n",
    "        model = model_bigru(num_hidden=num_hidden, use_sigmoid=use_sigmoid)   \n",
    "  \n",
    "        model.summary()\n",
    "        model.compile(optimizer=Adam(learning_rate=beta),\n",
    "            loss= 'binary_crossentropy',\n",
    "            metrics=['accuracy', AUC()])\n",
    "         \n",
    "        history = model.fit([x_train[:,:L].astype(int), x_train[:,L:]], t_train,\n",
    "                           validation_data=([x_valid[:,:L].astype(int), x_valid[:,L:]],t_valid),\n",
    "                           epochs = max_epochs,\n",
    "                           batch_size=batch_size,\n",
    "                           verbose=1,\n",
    "                           callbacks=callback1\n",
    "                       )\n",
    "        \n",
    "    keys = history.history.keys()\n",
    "    key_val_acc = get_key('val_acc', keys)\n",
    "    key_val_auc = get_key('val_auc', keys)\n",
    "    key_acc = get_key('acc', keys)\n",
    "    key_auc = get_key('auc', keys)    \n",
    "    \n",
    "    plt.figure(figsize=(9,6))\n",
    "    ep = np.arange(1,max_epochs+1)\n",
    "    plt.plot(ep, history.history[key_val_auc], 'r')\n",
    "    plt.xticks(np.arange(0,max_epochs+1,5, dtype=np.int))\n",
    "    plt.plot(ep, history.history[key_auc], 'b')\n",
    "    plt.plot(ep, history.history[key_val_acc], 'r:')\n",
    "    plt.plot(ep, history.history[key_acc], 'b:')\n",
    "    plt.legend(['val.auc', 'auc', 'val.acc', 'acc'])\n",
    "    plt.grid(b=True)\n",
    "    \n",
    "    if USE_W2V:\n",
    "        title=\"DATASET={},split={},L={}, embsize={}, w2v={}, layers={}, model={}\".format(\n",
    "                DATASET, fold, L, w2v_emb_size, True, num_hidden, mod)\n",
    "    else:\n",
    "        title=\"DATASET={},split={}, L={}, embsize={}, w2v={}, layers={}, model={}\".format(\n",
    "                DATASET, fold, L, emb_size, False, num_hidden, mod)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    all_val_auc = np.array(history.history[key_val_auc])\n",
    "    max_v_auc.append(max(all_val_auc))\n",
    "\n",
    "\n",
    "#plot the model\n",
    "if USE_W2V:\n",
    "    model_title=\"{}_L={}_embsize={}_w2v={}_layers={}_model={}.png\".format(\n",
    "               DATASET, L, w2v_emb_size, True, num_hidden,mod)\n",
    "else:\n",
    "    model_title=\"{}_L={}_embsize={}_w2v={}_layers={}_model={}.png\".format(\n",
    "                DATASET, L, emb_size, False, num_hidden,mod)\n",
    "        \n",
    "model_file = path.join(rootdir,DATASET,model_title)\n",
    "tf.keras.utils.plot_model(model, to_file=model_file, show_shapes=True)   \n",
    "\n",
    "#VALIDATION RESULT\n",
    "av_val_auc = Average(max_v_auc)\n",
    "print('Average validation auc of 5 folds cross-validation is {} '.format(av_val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLM6faB-W-6b",
    "outputId": "ca861cfc-68cc-4223-cfa4-3da4d5620228",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Train Test\n",
    "\n",
    "print(\" =============== START THE EXPERIMENT ============\")\n",
    "print(\"~~~~~~~~~~~~~~~DATASET {} ~~~~~~~~~~~\".format(DATASET))\n",
    "max_test_auc = []\n",
    "\n",
    "if USE_W2V:\n",
    "    data_file = path.join(rootdir,DATASET,\"{}_w2v_3lines_L{}_emb_size={}.npz\".format(DATASET,L,w2v_emb_size))\n",
    "else:\n",
    "    data_file = path.join(rootdir,DATASET,\"{}_no_w2v_3lines_L{}_emb_size={}.npz\".format(DATASET,L,emb_size))\n",
    "\n",
    "\n",
    "data_train, data_test, embeddings, skill_names, spatd, gausd = read_data_test(DATASET=DATASET)\n",
    "skill_ids = np.unique(np.hstack((\n",
    "    data_train[:,1],\n",
    "    data_test[:,1]\n",
    "    )))\n",
    "num_skills = len(skill_ids)\n",
    "train_user_ids = np.unique(data_train[:,0])\n",
    "test_user_ids = np.unique(data_test[:,0])\n",
    "N_train = len(train_user_ids)\n",
    "N_test = len(test_user_ids)\n",
    "num_students = N_train + N_test\n",
    "\n",
    "print('Number of skills: {}'.format(num_skills))\n",
    "print('Number of train students: {}'.format(N_train))\n",
    "print('Number of test students: {}'.format(N_test))\n",
    "print('(total: {})'.format(num_students))\n",
    "\n",
    "# ### Generate `x_train`, `x_test`, `t_train`, `t_test`  \n",
    "# Every student `stud_id` has a sequence of responses `correct[0], correct[1],..., correct[T-1]` for some skill `skill_id`. The length `T` of the sequence depends on the student and the skill.   \n",
    "# Every row of `x_train` or `x_test` contains the `student_id`, the `skill_id` and the response `correct[t]` for some time `t`. In addition to that it also includes the history of length `L` of previous responses `correct[t-1],..., correct[t-L]`. These responses *must* correspond to the same student and the same skill as time `t`. If history is shorter than `L` then the missing entries are filled with `0`.\n",
    "    \n",
    "if not path.exists(data_file):\n",
    "\n",
    "    #Generate Training, Validation and Testing data\n",
    "\n",
    "    x_train, t_train = gen_inputs_targets(data_train,\n",
    "                            train_user_ids, N_train, 'Train set:')\n",
    "    x_test, t_test = gen_inputs_targets(data_test,\n",
    "                            test_user_ids, N_test, 'Test set:')\n",
    "\n",
    "    np.savez(data_file, \\\n",
    "             embeddings = embeddings, \\\n",
    "             x_train = x_train, \\\n",
    "             x_test = x_test, \\\n",
    "             t_train = t_train, \\\n",
    "             t_test = t_test, \\\n",
    "             train_user_ids = train_user_ids, \\\n",
    "             test_user_ids = test_user_ids, \\\n",
    "             N_train = N_train, \\\n",
    "             N_test = N_test, \\\n",
    "             num_skills = num_skills, \\\n",
    "             num_students = num_students)    \n",
    "else:\n",
    "    data_split = np.load(data_file)\n",
    "    embeddings = data_split['embeddings']\n",
    "    x_train = data_split['x_train']\n",
    "    x_test = data_split['x_test']\n",
    "    t_train = data_split['t_train']\n",
    "    t_test = data_split['t_test']\n",
    "    train_user_ids = data_split['train_user_ids']\n",
    "    test_user_ids = data_split['test_user_ids']\n",
    "    N_train = data_split['N_train']\n",
    "    N_test = data_split['N_test']\n",
    "    num_skills = data_split['num_skills']\n",
    "    num_students = data_split['num_students']\n",
    "     \n",
    "#Train the model\n",
    "acc_test_base = np.sum(t_test==1)/t_test.shape[0]\n",
    "print('Baseline test accuracy = {}'.format(acc_test_base))   \n",
    "print(\"==================================================\")\n",
    "if USE_W2V:\n",
    "    print('L = {}, emb_size = {}, hidden={}, spatial dropout = {}, gaussian dropout = {}'.format(\n",
    "            L, w2v_emb_size, num_hidden, spatd, gausd))\n",
    "else:\n",
    "    print('L = {}, emb_size = {}, hidden={}, spatial dropout = {}, gaussian dropout = {}'.format(\n",
    "            L, emb_size, num_hidden, spatd, gausd))\n",
    "\n",
    "try:        \n",
    "    del model \n",
    "    K.clear_session()\n",
    "except:\n",
    "    print(\"no model to delete\")\n",
    "\n",
    "if mod == 'tdnn_model':    \n",
    "    print(\"~~~~~~~Train TDNN MODEL~~~~~~~~~~\")\n",
    "    model = model_tdnn(num_hidden=num_hidden, use_sigmoid=use_sigmoid)    \n",
    "\n",
    "    model.summary()   \n",
    "    model.compile(optimizer=Adamax(learning_rate=beta), \\\n",
    "                  loss='binary_crossentropy', \\\n",
    "                  metrics=['accuracy', AUC()])\n",
    "\n",
    "    history = model.fit([x_train[:,:L].astype(int), x_train[:,L:]], t_train, \\\n",
    "                        validation_data=([x_test[:,:L].astype(int), x_test[:,L:]],t_test), \\\n",
    "                        epochs = max_epochs, \\\n",
    "                        batch_size=batch_size, \\\n",
    "                        verbose=1)#, \n",
    "                       #callbacks=my_callbacks ) #[ReduceLROnPlateau()]) #, ), # MyCallback()])\n",
    "                        #)\n",
    "\n",
    "\n",
    "elif mod == 'bigru_model':    \n",
    "    print(\"~~~~~~~~~~Train Bi-GRU MODEL~~~~~~~~\")\n",
    "    model = model_bigru(num_hidden=num_hidden, use_sigmoid=use_sigmoid)   \n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(learning_rate=beta), \\\n",
    "        loss= 'binary_crossentropy', \\\n",
    "        metrics=['accuracy', AUC()])\n",
    "\n",
    "    history = model.fit([x_train[:,:L].astype(int), x_train[:,L:]], t_train, \\\n",
    "                        validation_data=([x_test[:,:L].astype(int), x_test[:,L:]],t_test), \\\n",
    "                        epochs = max_epochs, \\\n",
    "                        batch_size=batch_size, \\\n",
    "                        verbose=1, \\\n",
    "                        callbacks=callback1)\n",
    "\n",
    "#plot the model\n",
    "if USE_W2V:\n",
    "    model_title=\"{}_L={}_embsize={}_w2v={}_layers={}_model={}.png\".format( \\\n",
    "            DATASET, L, w2v_emb_size, True, num_hidden,mod)\n",
    "else:\n",
    "    model_title=\"{}_L={}_embsize={}_w2v={}_layers={}_model={}.png\".format( \\\n",
    "            DATASET, L, emb_size, False, num_hidden,mod)\n",
    "\n",
    "model_file = path.join(rootdir,DATASET,model_title)\n",
    "tf.keras.utils.plot_model(model, to_file=model_file, show_shapes=True)    \n",
    "\n",
    "keys = history.history.keys()\n",
    "key_val_acc = get_key('val_acc', keys)\n",
    "key_val_auc = get_key('val_auc', keys)\n",
    "key_acc = get_key('acc', keys)\n",
    "key_auc = get_key('auc', keys)\n",
    "\n",
    "\n",
    "print(\"!!!!!!!!!!!!!!!!! TEST RESULTS !!!!!!!!!!!!!!!!!!!!!\")\n",
    "all_test_auc = np.array(history.history[key_val_auc])\n",
    "max_test_auc.append(max(all_test_auc))\n",
    "av_test_auc = Average(all_test_auc)\n",
    "\n",
    "print(\" =============== RESULTS OF EXPERIMENT  ============\")\n",
    "print(\"Max test auc  is {} \".format(max(all_test_auc)))\n",
    "print('Average test auc  is {} '.format(av_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"~~~~~EVALUATION RESULTS ~~~~~~~\")\n",
    "model.evaluate([x_test[:,:L].astype(int), x_test[:,L:].astype(int)],\n",
    "                            t_test,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "ep = np.arange(1,max_epochs+1)\n",
    "plt.plot(ep, history.history[key_val_auc], 'r')\n",
    "plt.xticks(np.arange(0,max_epochs+1,5, dtype=np.int))\n",
    "plt.plot(ep, history.history[key_auc], 'b')\n",
    "plt.plot(ep, history.history[key_val_acc], 'r:')\n",
    "plt.plot(ep, history.history[key_acc], 'b:')\n",
    "plt.legend(['test.auc', 'auc', 'test.acc', 'acc'])\n",
    "plt.grid(b=True)\n",
    "if USE_W2V:\n",
    "    title=\"model={},DATASET={},L={}, embsize={}, w2v={}, layers={}\".format(\n",
    "                mod, DATASET, L, w2v_emb_size, True, num_hidden)\n",
    "else:\n",
    "    title=\"model={},DATASET={}, L={}, embsize={}, w2v={}, layers={}\".format(\n",
    "            mod, DATASET, L, emb_size, False, num_hidden)\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([x_test[:,:L].astype(int),x_test[:,L:].astype(int)],verbose=1)\n",
    "\n",
    "if mod == 'tdnn_model': \n",
    "    print('Tdnn predictions')\n",
    "    print(preds)\n",
    "    preds.tofile(rootdir+\"/\"+DATASET+\"/Tdnn_predictions.csv\",sep=',')\n",
    "    preds_bin = preds\n",
    "    preds_bin[preds_bin > 0.5] = 1.0\n",
    "    preds_bin[preds_bin <= 0.5] = 0.0\n",
    "    print(\"Tdnn binary predictions\")\n",
    "    print(preds_bin)        \n",
    "    preds_bin.tofile(rootdir+\"/\"+DATASET+\"/Tdnn_binary_predictions.csv\",sep=',')\n",
    "    print('Tdnn targets')\n",
    "    print(t_test)\n",
    "else:   \n",
    "    print('BiGru predictions')\n",
    "    print(preds)    \n",
    "    preds.tofile(rootdir+\"/\" +DATASET+ \"/BiGru_predictions.csv\",sep=',')\n",
    "    preds_bin = preds\n",
    "    preds_bin[preds_bin > 0.5] = 1.0\n",
    "    preds_bin[preds_bin <= 0.5] = 0.0\n",
    "    print(\"BiGru binary predictions\")\n",
    "    print(preds_bin)    \n",
    "    preds_bin.tofile(rootdir+\"/\" +DATASET+ \"/BiGru_binary_predictions.csv\",sep=',')\n",
    "    print('BiGru targets')\n",
    "    print(t_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPeRPhG+pQJ0G00gIzK/0PE",
   "collapsed_sections": [],
   "name": "tdnn-conv_edm_hist_emb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
